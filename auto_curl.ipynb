{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import urllib\n",
    "import http.cookiejar\n",
    "import time\n",
    "\n",
    "urlroot='http://www.toyodiy.com/parts/'\n",
    "urlpage=[]\n",
    "\n",
    "#KUN26L-PRMDYG and -HRMDY (one comes from Argentina and the other from Thailand). \n",
    "#The European equivalent is KUN26L-PRMDHW\n",
    "\n",
    "urlpage.append('g_G_2015_TOYOTA_HILUX_KUN26L-PRMDYG.html')\n",
    "urlpage.append('g_E_2015_TOYOTA_HILUX_KUN26R-PRMDYW.html')\n",
    "\n",
    "#urlpage.append('g_G_2015_TOYOTA_HILUX_KUN26L-PRMDYG.html')\n",
    "#urlpage.append('g_E_2005_TOYOTA_LAND+CRUISER+PRADO_KDJ120L-GKMEYW.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_url(url):\n",
    "    cj = http.cookiejar.CookieJar()\n",
    "    opener = urllib.request.build_opener(urllib.request.HTTPCookieProcessor(cj))\n",
    "    request = urllib.request.Request(url)\n",
    "    response = opener.open(request)\n",
    "    return response.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_url_list(url):\n",
    "    href=[]\n",
    "    \n",
    "    html_string = open_url(url)\n",
    "    print (html_string)\n",
    "    soup = BeautifulSoup(html_string, 'html.parser')\n",
    "\n",
    "    soup=soup.find('div', attrs={'class': 'msg1 vs2'})\n",
    "    \n",
    "    for link in soup.find_all('a'):\n",
    "        href.append(link.get('href'))\n",
    "    \n",
    "    return href"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_parts(url):\n",
    "\n",
    "    html_string = open_url(url)\n",
    "    soup = BeautifulSoup(html_string, 'html.parser')\n",
    "\n",
    "    href=[]\n",
    "    txt=[]\n",
    "\n",
    "    for link in soup.find('div', attrs={'class': 'diag-list'}):\n",
    "        href.append(link.find('a')['href'])\n",
    "        txt.append(link.find('a').text)\n",
    "\n",
    "    df = pd.DataFrame({'Description': [], 'Part_Number' : []})\n",
    "    #keylist = list(df.columns.values) --- this worked in a paragraph but not once in a function!!\n",
    "    keylist=df.columns.values.tolist()\n",
    "\n",
    "    for i in href:\n",
    "        page = open_url(urlroot+i)\n",
    "    \n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "    \n",
    "        n=-1\n",
    "        dict={}\n",
    "        for i in soup.findAll('tbody')[0].findAll('tr'):\n",
    "            n=n+1\n",
    "            lim=i.findAll('td')[0].text\n",
    "            if n==0:\n",
    "            ### DESCRIPTION FIELD ###\n",
    "            #if description starts with 5 digits and a -, then this is a part associated with description on line above\n",
    "            #this is slicing syntax - slices are the places between the characters\n",
    "                if 'STD. PART' in lim:\n",
    "                        desc='STD. PART'\n",
    "                        lim=lim[0:11]\n",
    "                        n=2\n",
    "                else:\n",
    "                    if lim[0:5].isdigit() and lim[5:6]=='-':\n",
    "                        lim=lim[0:11]\n",
    "                        n=2\n",
    "                    else:\n",
    "                        lim=i.findAll('td')[1].text\n",
    "                        start=re.search('\\D', lim).start()\n",
    "                        lim=lim[start:]\n",
    "                        desc=lim\n",
    "            if n==1:\n",
    "            #grab 11 chars which should be part number\n",
    "                lim=lim[0:11]\n",
    "            if n!=2:\n",
    "                dict[keylist[n]]=lim\n",
    "            else:\n",
    "                dict[keylist[0]]=desc\n",
    "                dict[keylist[1]]=lim\n",
    "            if n==1:\n",
    "                df = df.append(dict, ignore_index=True)\n",
    "                dict.clear()\n",
    "                n=-1\n",
    "            if n==2:\n",
    "                df = df.append(dict, ignore_index=True)\n",
    "                dict.clear()\n",
    "                n=-1\n",
    "    \n",
    "        #pause to prevent auto download detection\n",
    "        time.sleep(1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllist=get_url_list(urlroot+urlpage[0])\n",
    "\n",
    "print(urllist[0])\n",
    "df1=generate_parts(urlroot+urllist[0])\n",
    "print(urllist[1])\n",
    "df2=generate_parts(urlroot+urllist[1])\n",
    "print(urllist[2])\n",
    "df3=generate_parts(urlroot+urllist[2])\n",
    "print(urllist[3])\n",
    "df4=generate_parts(urlroot+urllist[3])\n",
    "\n",
    "urllist=get_url_list(urlroot+urlpage[1])\n",
    "\n",
    "print(urllist[0])\n",
    "df5=generate_parts(urlroot+urllist[0])\n",
    "print(urllist[1])\n",
    "df6=generate_parts(urlroot+urllist[1])\n",
    "print(urllist[2])\n",
    "df7=generate_parts(urlroot+urllist[2])\n",
    "print(urllist[3])\n",
    "df8=generate_parts(urlroot+urllist[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigdf1=df1.append([df2,df3,df4],ignore_index=True)\n",
    "bigdf2=df5.append([df6,df7,df8],ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigdf1=bigdf1.append({'Description':'TEST ITEM', 'Part_Number':'1234'}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#indicator=True gives _merge column will show left_only if only left side exists in an outer join\n",
    "df=pd.merge(bigdf1,bigdf2,on=['Part_Number'],how=\"outer\",indicator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfleft=df[df['_merge']=='left_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfright=df[df['_merge']=='right_only']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfleft.to_csv('arg.csv')\n",
    "dfright.to_csv('uk.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
